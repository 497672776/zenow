# -*- coding: utf-8 -*-
"""
Zenow Backend Main Application
ä½¿ç”¨ APIRouter é‡æ„åçš„ä¸»åº”ç”¨æ–‡ä»¶
"""

import logging

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# å¯¼å…¥æ‰€æœ‰è·¯ç”±
from routers import (
    models_router,
    sessions_router,
    chat_router,
    system_router
)

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from spacemit_llm.model.server_manager import ModelServerManager
from spacemit_llm.model.download import ModelDownloader
from spacemit_llm.comon.sqlite.sqlite_config import SQLiteConfig
from spacemit_llm.comon.sqlite.sqlite_session import SQLiteSession
from spacemit_llm.pipeline.model_select import ModelSelectionPipeline
from spacemit_llm.pipeline.backend_start import BackendStartupHandler
from spacemit_llm.pipeline.model_param_change import ModelParameterChangePipeline
from spacemit_llm.pipeline.chat import ChatPipeline
from utils.port import write_port_file, cleanup_port_file
import config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# å…¨å±€ç»„ä»¶åˆå§‹åŒ–
# ============================================================================

# æ•°æ®åº“
db_config = SQLiteConfig(config.DB_CONFIG_PATH)
db_session = SQLiteSession(config.DB_SESSION_PATH)

# æ¨¡å‹æœåŠ¡å™¨ç®¡ç†
server_manager = ModelServerManager()

# æ¨¡å‹ä¸‹è½½å™¨
model_downloader = ModelDownloader(config.LLM_MODELS_DIR)

# Pipeline ç»„ä»¶
model_selection_pipeline = ModelSelectionPipeline(
    models_dir=config.LLM_MODELS_DIR,
    downloader=model_downloader,
    server_manager=server_manager,
    db_config=db_config
)

# Register startup handler to initialize database and start all current models
startup_handler = BackendStartupHandler(server_manager, db_config, config)

# Register parameter change pipeline
param_change_pipeline = ModelParameterChangePipeline(
    server_manager.get_server("llm"),
    server_manager.get_client("llm"),
    db_config
)

# Initialize chat pipeline
chat_pipeline = ChatPipeline(
    server_manager,
    db_config,
    db_session,
    default_system_prompt=config.DEFAULT_SYSTEM_PROMPT,
    default_context_size=config.LLM_SERVER_CONTEXT_SIZE
)

# ============================================================================
# è®¾ç½®è·¯ç”±ä¾èµ–
# ============================================================================

# è®¾ç½® models router çš„å…¨å±€å˜é‡
models_router.server_manager = server_manager
models_router.model_downloader = model_downloader
models_router.db_config = db_config
models_router.model_selection_pipeline = model_selection_pipeline
models_router.param_change_pipeline = param_change_pipeline

# è®¾ç½® sessions router çš„å…¨å±€å˜é‡
sessions_router.db_session = db_session

# è®¾ç½® chat router çš„å…¨å±€å˜é‡
chat_router.chat_pipeline = chat_pipeline

# ============================================================================
# FastAPI åº”ç”¨é…ç½®
# ============================================================================

app = FastAPI(
    title="Zenow API",
    description="Zenow LLM Chat Application API",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œæ‰€æœ‰è·¯ç”±
app.include_router(system_router)    # ç³»ç»Ÿè·¯ç”±ï¼ˆåŒ…å«æ ¹è·¯å¾„å’Œå¥åº·æ£€æŸ¥ï¼‰
app.include_router(models_router)    # æ¨¡å‹ç®¡ç†è·¯ç”±
app.include_router(sessions_router)  # ä¼šè¯ç®¡ç†è·¯ç”±
app.include_router(chat_router)      # èŠå¤©è·¯ç”±

# ============================================================================
# åº”ç”¨ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ğŸš€ Starting Zenow Backend...")

    # å†™å…¥ç«¯å£æ–‡ä»¶
    write_port_file(config.API_SERVER_PORT)

    # åˆå§‹åŒ–æ•°æ®åº“å’Œå¯åŠ¨æ¨¡å‹
    await startup_handler.initialize()

    logger.info("âœ… Zenow Backend started successfully")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    logger.info("ğŸ›‘ Shutting down Zenow Backend...")

    # åœæ­¢æ‰€æœ‰ llama-server è¿›ç¨‹
    try:
        await server_manager.stop_all()
        logger.info("âœ“ All llama-server processes stopped")
    except Exception as e:
        logger.warning(f"Async cleanup failed: {e}, trying synchronous cleanup")
        server_manager.stop_all_sync()
        logger.info("âœ“ All llama-server processes stopped (sync)")

    # æ¸…ç†ç«¯å£æ–‡ä»¶
    cleanup_port_file()

    logger.info("âœ… Zenow Backend shutdown complete")

# ============================================================================
# åº”ç”¨å¯åŠ¨
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=config.API_SERVER_HOST, port=config.API_SERVER_PORT)