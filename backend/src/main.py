# -*- coding: utf-8 -*-
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json
import os
import asyncio
import logging
from pathlib import Path
from dotenv import load_dotenv

from src.model.llm import LLMServer, LLMClient
from src.model.server_manager import ModelServerManager
from src.model.download import ModelDownloader
from src.comon.sqlite.sqlite_config import SQLiteConfig
from src.comon.sqlite.sqlite_session import SQLiteSession
from src.pipeline.model_select import ModelSelectionPipeline
from src.pipeline.backend_exit import BackendExitHandler
from src.pipeline.backend_start import BackendStartupHandler
from src.pipeline.model_param_change import ModelParameterChangePipeline
from src.utils.token_estimator import estimate_message_tokens
from src import config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()


def write_port_file(port: int):
    """Write the port number to a file for frontend to read."""
    port_file = Path.home() / ".config" / "zenow" / "backend.port"
    port_file.parent.mkdir(parents=True, exist_ok=True)

    # Write port with PID to detect stale files
    port_file.write_text(f"{port}\n{os.getpid()}\n")
    logger.info(f"ğŸ“ Wrote port {port} and PID {os.getpid()} to {port_file}")


def cleanup_port_file():
    """Remove port file on shutdown."""
    port_file = Path.home() / ".config" / "zenow" / "backend.port"
    if port_file.exists():
        port_file.unlink()
        logger.info(f"ğŸ§¹ Cleaned up port file: {port_file}")


app = FastAPI(title="Zenow LLM Chat API")

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize on startup"""
    write_port_file(config.API_SERVER_PORT)

    # Initialize database and start model
    await startup_handler.initialize()


@app.on_event("shutdown")
async def shutdown_event():
    """Clean up port file on shutdown."""
    cleanup_port_file()


# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global instances
db_config = SQLiteConfig(config.DB_CONFIG_PATH)
db_session = SQLiteSession(config.DB_SESSION_PATH)

# ä½¿ç”¨ ModelServerManager ç®¡ç†å¤šä¸ªæœåŠ¡å™¨å®ä¾‹
server_manager = ModelServerManager(
    host=config.LLM_SERVER_HOST,
    llm_port=config.LLM_SERVER_PORT,
    embed_port=config.EMBED_SERVER_PORT,
    rerank_port=config.RERANK_SERVER_PORT,
    context_size=config.LLM_SERVER_CONTEXT_SIZE,
    threads=config.LLM_SERVER_THREADS,
    gpu_layers=config.LLM_SERVER_GPU_LAYERS,
    batch_size=config.LLM_SERVER_BATCH_SIZE
)

# è·å– LLM æ¨¡å¼çš„æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
llm_server = server_manager.get_server("llm")
llm_client = server_manager.get_client("llm")

model_downloader = ModelDownloader(config.LLM_MODELS_DIR)
model_selection_pipeline = ModelSelectionPipeline(
    models_dir=config.LLM_MODELS_DIR,
    downloader=model_downloader,
    server_manager=server_manager,
    db_config=db_config
)

# Register exit handler to clean up all llama-servers on backend exit
exit_handler = BackendExitHandler(server_manager)
exit_handler.register()

# Register startup handler to initialize database and start all current models
startup_handler = BackendStartupHandler(server_manager, db_config, config)

# Register parameter change pipeline
param_change_pipeline = ModelParameterChangePipeline(llm_server, llm_client, db_config)

# Models
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    repeat_penalty: Optional[float] = None
    max_tokens: Optional[int] = None
    mode: Optional[str] = "llm"  # Added mode parameter
    session_id: Optional[int] = None  # Optional session ID for history management

class ChatResponse(BaseModel):
    message: Message
    model: str

class ModelInfo(BaseModel):
    id: int
    name: str
    path: str
    is_downloaded: bool
    mode: Optional[str] = "llm"  # Added mode field

class ModelListResponse(BaseModel):
    models: List[ModelInfo]
    current_model: Optional[ModelInfo] = None

class AddModelRequest(BaseModel):
    name: str
    path: str
    mode: Optional[str] = "llm"  # Added mode parameter

class ServerStatusResponse(BaseModel):
    status: str
    model_name: Optional[str] = None
    model_path: Optional[str] = None
    is_running: bool
    error_message: Optional[str] = None

class DownloadModelRequest(BaseModel):
    url: str
    filename: Optional[str] = None
    mode: Optional[str] = "llm"  # Added mode parameter

class DownloadStatusResponse(BaseModel):
    url: str
    filename: str
    status: str  # downloading, completed, failed
    downloaded: int
    total: int
    progress: float
    error: Optional[str] = None

class DefaultDownloadUrlsResponse(BaseModel):
    urls: Dict[str, List[str]]  # Changed to dict by mode
    browser_path: str

class SelectModelRequest(BaseModel):
    model_name: str
    download_url: Optional[str] = None
    mode: Optional[str] = "llm"  # Added mode parameter

class SelectModelResponse(BaseModel):
    success: bool
    message: str
    model_name: str
    model_path: Optional[str]
    server_status: str

class LLMParametersRequest(BaseModel):
    # LLMServer å‚æ•°
    context_size: Optional[int] = None
    threads: Optional[int] = None
    gpu_layers: Optional[int] = None
    batch_size: Optional[int] = None
    # LLMClient å‚æ•°
    temperature: Optional[float] = None
    repeat_penalty: Optional[float] = None
    max_tokens: Optional[int] = None

class LLMParametersResponse(BaseModel):
    # LLMServer å‚æ•°
    context_size: int
    threads: int
    gpu_layers: int
    batch_size: int
    # LLMClient å‚æ•°
    temperature: float
    repeat_penalty: float
    max_tokens: int

class UpdateParametersResponse(BaseModel):
    success: bool
    message: str
    requires_restart: bool

# ==================== Session Management Models ====================

class SessionInfo(BaseModel):
    id: int
    session_name: str
    created_at: str
    updated_at: str
    message_count: int
    total_tokens: int

class SessionListResponse(BaseModel):
    sessions: List[SessionInfo]
    total: int

class CreateSessionRequest(BaseModel):
    first_message: str

class CreateSessionResponse(BaseModel):
    session_id: int
    session_name: str

class UpdateSessionNameRequest(BaseModel):
    new_name: str

class MessageInfo(BaseModel):
    id: int
    session_id: int
    role: str
    content: str
    token_count: int
    created_at: str

class MessagesResponse(BaseModel):
    messages: List[MessageInfo]
    session_id: int
    total_tokens: int

class AddMessageRequest(BaseModel):
    role: str
    content: str

class AddMessageResponse(BaseModel):
    message_id: int
    token_count: int



@app.get("/")
async def root():
    return {"message": "Zenow LLM Chat API"}


# ============================================================================
# æ ¸å¿ƒ API ç«¯ç‚¹ (6ä¸ª)
# ============================================================================

@app.get("/api/models/current")
async def get_current_model(mode: str = "llm"):
    """
    è·å–å½“å‰æ¨¡å‹

    Args:
        mode: æ¨¡å‹æ¨¡å¼ ('llm', 'embed', 'rerank')

    Returns:
        å½“å‰æ¨¡å‹ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    try:
        current = db_config.get_current_model(mode=mode)
        if current:
            # æ£€æŸ¥å¹¶æ›´æ–°ä¸‹è½½çŠ¶æ€
            db_config.check_and_update_download_status(current["id"])
            # é‡æ–°è·å–æ›´æ–°åçš„çŠ¶æ€
            current = db_config.get_current_model(mode=mode)

            return {
                "id": current["id"],
                "name": current["model_name"],
                "path": current["model_path"],
                "is_downloaded": bool(current.get("is_downloaded", False)),
                "mode": current.get("mode", mode)
            }
        else:
            return None
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/list")
async def get_models_list(mode: Optional[str] = None):
    """
    è·å–æ¨¡å‹åˆ—è¡¨

    Args:
        mode: å¯é€‰ï¼Œç­›é€‰ç‰¹å®šæ¨¡å¼çš„æ¨¡å‹ ('llm', 'embed', 'rerank')

    Returns:
        æ¨¡å‹åˆ—è¡¨å’Œå½“å‰æ¨¡å‹
    """
    try:
        models = db_config.get_all_models(mode=mode)
        current = db_config.get_current_model(mode=mode or "llm") if mode else None

        # æ›´æ–°æ‰€æœ‰æ¨¡å‹çš„ä¸‹è½½çŠ¶æ€
        for model in models:
            db_config.check_and_update_download_status(model["id"])

        # é‡æ–°è·å–æ›´æ–°åçš„æ¨¡å‹åˆ—è¡¨
        models = db_config.get_all_models(mode=mode)

        model_list = [
            {
                "id": m["id"],
                "name": m["model_name"],
                "path": m["model_path"],
                "is_downloaded": bool(m.get("is_downloaded", False)),
                "mode": m.get("mode", "llm")
            }
            for m in models
        ]

        current_model_info = None
        if current:
            current_model_info = {
                "id": current["id"],
                "name": current["model_name"],
                "path": current["model_path"],
                "is_downloaded": bool(current.get("is_downloaded", False)),
                "mode": current.get("mode", "llm")
            }

        return {
            "models": model_list,
            "current_model": current_model_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/models/load")
async def load_model(request: SelectModelRequest):
    """
    åŠ è½½æ¨¡å‹ï¼ˆä¸‹è½½ + åˆ‡æ¢ + å¯åŠ¨æœåŠ¡å™¨ï¼‰

    Args:
        request: åŒ…å« model_name, download_url (å¯é€‰), mode (å¯é€‰)

    Returns:
        æ“ä½œç»“æœ
    """
    try:
        result = await model_selection_pipeline.select_model(
            model_name=request.model_name,
            download_url=request.download_url,
            mode=request.mode or "llm"
        )
        return result
    except Exception as e:
        logger.error(f"Model load failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/models/download")
async def download_model(request: DownloadModelRequest):
    """
    ä¸‹è½½æ¨¡å‹ï¼ˆä»…ä¸‹è½½ï¼Œä¸åˆ‡æ¢ï¼‰

    Args:
        request: åŒ…å« url, filename (å¯é€‰), mode (å¯é€‰)

    Returns:
        ä¸‹è½½çŠ¶æ€
    """
    try:
        mode = request.mode or "llm"
        from src.config import get_model_dir

        # è·å–æ¨¡å¼å¯¹åº”çš„ç›®å½•
        mode_dir = get_model_dir(mode)

        # ä½¿ç”¨å…¨å±€ model_downloaderï¼Œä¸´æ—¶åˆ‡æ¢ç›®å½•
        original_dir = model_downloader.download_dir
        model_downloader.download_dir = mode_dir

        # å¼€å§‹ä¸‹è½½ï¼ˆå¼‚æ­¥ï¼‰
        asyncio.create_task(
            model_downloader.download_model(
                url=request.url,
                filename=request.filename
            )
        )

        # æ¢å¤åŸç›®å½•
        model_downloader.download_dir = original_dir

        return {
            "success": True,
            "url": request.url,
            "mode": mode,
            "message": "Download started"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/download/status")
async def get_download_status(url: Optional[str] = None):
    """
    è·å–ä¸‹è½½è¿›åº¦çŠ¶æ€

    Args:
        url: å¯é€‰ï¼ŒæŒ‡å®š URL çš„ä¸‹è½½çŠ¶æ€ã€‚å¦‚æœä¸æä¾›ï¼Œè¿”å›æ‰€æœ‰ä¸‹è½½çŠ¶æ€

    Returns:
        ä¸‹è½½çŠ¶æ€ä¿¡æ¯
    """
    try:
        if url:
            # è·å–ç‰¹å®š URL çš„ä¸‹è½½çŠ¶æ€
            status = model_downloader.get_download_status(url)
            if status:
                return {
                    "success": True,
                    "download": status
                }
            else:
                return {
                    "success": False,
                    "message": "Download not found"
                }
        else:
            # è·å–æ‰€æœ‰ä¸‹è½½çŠ¶æ€
            all_downloads = model_downloader.get_all_downloads()
            return {
                "success": True,
                "downloads": all_downloads
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/get_param")
async def get_model_parameters(mode: str = "llm"):
    """
    è·å–æ¨¡å‹å‚æ•°

    Args:
        mode: æ¨¡å‹æ¨¡å¼ ('llm', 'embed', 'rerank')

    Returns:
        æ¨¡å‹å‚æ•°é…ç½®
    """
    try:
        if mode == "llm":
            server = server_manager.get_server("llm")
            client = server_manager.get_client("llm")

            return {
                # LLMServer å‚æ•°
                "context_size": server.context_size,
                "threads": server.threads,
                "gpu_layers": server.gpu_layers,
                "batch_size": server.batch_size,
                # LLMClient å‚æ•°
                "temperature": client.temperature,
                "repeat_penalty": client.repeat_penalty,
                "max_tokens": client.max_tokens
            }
        elif mode == "embed":
            server = server_manager.get_server("embed")
            client = server_manager.get_client("embed")

            return {
                # EmbedServer å‚æ•°
                "context_size": server.context_size,
                "threads": server.threads,
                "gpu_layers": server.gpu_layers,
                "batch_size": server.batch_size,
                # EmbedClient å‚æ•°
                "normalize": client.normalize,
                "truncate": client.truncate
            }
        elif mode == "rerank":
            server = server_manager.get_server("rerank")
            client = server_manager.get_client("rerank")

            return {
                # RerankServer å‚æ•°
                "context_size": server.context_size,
                "threads": server.threads,
                "gpu_layers": server.gpu_layers,
                "batch_size": server.batch_size,
                # RerankClient å‚æ•°
                "top_n": client.top_n,
                "return_documents": client.return_documents
            }
        else:
            raise HTTPException(status_code=400, detail=f"Invalid mode: {mode}")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/models/update_param")
async def update_model_parameters(request: Dict[str, Any], mode: str = "llm"):
    """
    æ›´æ–°æ¨¡å‹å‚æ•°

    Args:
        request: å‚æ•°å­—å…¸
        mode: æ¨¡å‹æ¨¡å¼ ('llm', 'embed', 'rerank')

    Returns:
        æ›´æ–°ç»“æœ
    """
    try:
        if mode == "llm":
            # ä½¿ç”¨ç°æœ‰çš„ parameter change pipeline
            result = await param_change_pipeline.apply_parameters(
                context_size=request.get("context_size"),
                threads=request.get("threads"),
                gpu_layers=request.get("gpu_layers"),
                batch_size=request.get("batch_size"),
                temperature=request.get("temperature"),
                repeat_penalty=request.get("repeat_penalty"),
                max_tokens=request.get("max_tokens")
            )
            return result
        elif mode == "embed":
            server = server_manager.get_server("embed")
            client = server_manager.get_client("embed")

            # æ›´æ–°æœåŠ¡å™¨å‚æ•°
            server_params_changed = False
            if any(k in request for k in ["context_size", "threads", "gpu_layers", "batch_size"]):
                await server.update_params(
                    context_size=request.get("context_size"),
                    threads=request.get("threads"),
                    gpu_layers=request.get("gpu_layers"),
                    batch_size=request.get("batch_size")
                )
                server_params_changed = True

            # æ›´æ–°å®¢æˆ·ç«¯å‚æ•°
            if any(k in request for k in ["normalize", "truncate"]):
                client.update_parameters(
                    normalize=request.get("normalize"),
                    truncate=request.get("truncate")
                )

            return {
                "success": True,
                "message": "Parameters updated successfully",
                "requires_restart": server_params_changed
            }
        elif mode == "rerank":
            server = server_manager.get_server("rerank")
            client = server_manager.get_client("rerank")

            # æ›´æ–°æœåŠ¡å™¨å‚æ•°
            server_params_changed = False
            if any(k in request for k in ["context_size", "threads", "gpu_layers", "batch_size"]):
                await server.update_params(
                    context_size=request.get("context_size"),
                    threads=request.get("threads"),
                    gpu_layers=request.get("gpu_layers"),
                    batch_size=request.get("batch_size")
                )
                server_params_changed = True

            # æ›´æ–°å®¢æˆ·ç«¯å‚æ•°
            if any(k in request for k in ["top_n", "return_documents"]):
                client.update_parameters(
                    top_n=request.get("top_n"),
                    return_documents=request.get("return_documents")
                )

            return {
                "success": True,
                "message": "Parameters updated successfully",
                "requires_restart": server_params_changed
            }
        else:
            raise HTTPException(status_code=400, detail=f"Invalid mode: {mode}")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to update parameters: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# è¾…åŠ© API ç«¯ç‚¹ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§å’ŒåŠŸèƒ½æ”¯æŒï¼‰
# ============================================================================

@app.get("/api/server/status")
async def get_server_status(mode: str = "llm"):
    """
    è·å–æœåŠ¡å™¨çŠ¶æ€

    Args:
        mode: æ¨¡å‹æ¨¡å¼ ('llm', 'embed', 'rerank')
    """
    try:
        server = server_manager.get_server(mode)
        status = server.get_status()
        return status
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/downloads/default-urls")
async def get_default_download_urls():
    """è·å–é»˜è®¤æ¨¡å‹ä¸‹è½½ URLs"""
    return {
        "urls": config.DEFAULT_MODEL_DOWNLOAD_URLS,
        "browser_path": config.DEFAULT_MODEL_BROWSER_PATH
    }


# ==================== Session Management APIs ====================

@app.post("/api/sessions", response_model=CreateSessionResponse)
async def create_session(request: CreateSessionRequest):
    """
    åˆ›å»ºæ–°ä¼šè¯

    Args:
        request: åŒ…å«ç”¨æˆ·ç¬¬ä¸€æ¡æ¶ˆæ¯

    Returns:
        æ–°ä¼šè¯çš„ ID å’Œåç§°
    """
    try:
        session_id = db_session.create_session(request.first_message)
        session = db_session.get_session(session_id)

        return CreateSessionResponse(
            session_id=session_id,
            session_name=session['session_name']
        )
    except Exception as e:
        logger.error(f"Failed to create session: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/sessions", response_model=SessionListResponse)
async def get_sessions(limit: int = 100, offset: int = 0):
    """
    è·å–ä¼šè¯åˆ—è¡¨ï¼ŒæŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—

    Args:
        limit: è¿”å›çš„æœ€å¤§ä¼šè¯æ•°
        offset: åç§»é‡ï¼ˆç”¨äºåˆ†é¡µï¼‰

    Returns:
        ä¼šè¯åˆ—è¡¨
    """
    try:
        sessions = db_session.get_all_sessions(limit=limit, offset=offset)

        return SessionListResponse(
            sessions=[SessionInfo(**s) for s in sessions],
            total=len(sessions)
        )
    except Exception as e:
        logger.error(f"Failed to get sessions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/sessions/{session_id}")
async def get_session(session_id: int):
    """
    è·å–ä¼šè¯è¯¦æƒ…

    Args:
        session_id: ä¼šè¯ ID

    Returns:
        ä¼šè¯ä¿¡æ¯
    """
    try:
        session = db_session.get_session(session_id)

        if not session:
            raise HTTPException(status_code=404, detail="Session not found")

        return SessionInfo(**session)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get session: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/api/sessions/{session_id}/name")
async def update_session_name(session_id: int, request: UpdateSessionNameRequest):
    """
    æ›´æ–°ä¼šè¯åç§°

    Args:
        session_id: ä¼šè¯ ID
        request: åŒ…å«æ–°çš„ä¼šè¯åç§°

    Returns:
        æˆåŠŸçŠ¶æ€
    """
    try:
        success = db_session.update_session_name(session_id, request.new_name)

        if not success:
            raise HTTPException(status_code=404, detail="Session not found")

        return {"success": True, "message": "Session name updated"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update session name: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/sessions/{session_id}")
async def delete_session(session_id: int):
    """
    åˆ é™¤ä¼šè¯ï¼ˆä¼šçº§è”åˆ é™¤æ‰€æœ‰æ¶ˆæ¯ï¼‰

    Args:
        session_id: ä¼šè¯ ID

    Returns:
        æˆåŠŸçŠ¶æ€
    """
    try:
        success = db_session.delete_session(session_id)

        if not success:
            raise HTTPException(status_code=404, detail="Session not found")

        return {"success": True, "message": "Session deleted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete session: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/sessions/{session_id}/messages", response_model=MessagesResponse)
async def get_messages(session_id: int, limit: Optional[int] = None):
    """
    è·å–ä¼šè¯çš„æ¶ˆæ¯åˆ—è¡¨

    Args:
        session_id: ä¼šè¯ ID
        limit: å¯é€‰ï¼Œé™åˆ¶è¿”å›çš„æ¶ˆæ¯æ•°é‡ï¼ˆæœ€æ–°çš„ N æ¡ï¼‰

    Returns:
        æ¶ˆæ¯åˆ—è¡¨
    """
    try:
        messages = db_session.get_messages(session_id, limit=limit)
        total_tokens = db_session.get_session_token_count(session_id)

        return MessagesResponse(
            messages=[MessageInfo(**m) for m in messages],
            session_id=session_id,
            total_tokens=total_tokens
        )
    except Exception as e:
        logger.error(f"Failed to get messages: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/sessions/{session_id}/messages", response_model=AddMessageResponse)
async def add_message(session_id: int, request: AddMessageRequest):
    """
    æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯

    Args:
        session_id: ä¼šè¯ ID
        request: åŒ…å«è§’è‰²å’Œå†…å®¹

    Returns:
        æ–°æ¶ˆæ¯çš„ ID å’Œ token æ•°
    """
    try:
        # éªŒè¯ä¼šè¯æ˜¯å¦å­˜åœ¨
        session = db_session.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")

        # ä¼°ç®— token æ•°
        token_count = estimate_message_tokens(request.role, request.content)

        # æ·»åŠ æ¶ˆæ¯
        message_id = db_session.add_message(
            session_id=session_id,
            role=request.role,
            content=request.content,
            token_count=token_count
        )

        return AddMessageResponse(
            message_id=message_id,
            token_count=token_count
        )
    except HTTPException:
        raise
    except ValueError as e:
        # è§’è‰²éªŒè¯å¤±è´¥
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to add message: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/sessions/{session_id}/messages")
async def clear_messages(session_id: int):
    """
    æ¸…ç©ºä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯

    Args:
        session_id: ä¼šè¯ ID

    Returns:
        æˆåŠŸçŠ¶æ€
    """
    try:
        success = db_session.clear_session_messages(session_id)

        if not success:
            raise HTTPException(status_code=404, detail="Session not found")

        return {"success": True, "message": "Messages cleared"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to clear messages: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£ï¼ˆæµå¼å“åº”ï¼‰

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. æ— ä¼šè¯æ¨¡å¼ï¼ˆsession_id=Noneï¼‰ï¼šç›´æ¥å‘é€æ¶ˆæ¯ï¼Œä¸ä¿å­˜å†å²
    2. ä¼šè¯æ¨¡å¼ï¼ˆsession_idæä¾›ï¼‰ï¼šåŠ è½½å†å²è®°å½•ï¼Œä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“

    Args:
        request: åŒ…å« messages, mode (å¯é€‰), temperature (å¯é€‰), session_id (å¯é€‰) ç­‰
    """
    try:
        mode = request.mode or "llm"

        # è·å–å¯¹åº”æ¨¡å¼çš„æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯
        server = server_manager.get_server(mode)
        client = server_manager.get_client(mode)

        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
        status = server.get_status()
        if not status["is_running"]:
            raise HTTPException(
                status_code=400,
                detail=f"{mode.upper()} server is not running. Please select a model first."
            )

        # å‡†å¤‡å‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨
        messages_to_send = []

        # ä¼šè¯æ¨¡å¼ï¼šåŠ è½½å†å²è®°å½•
        if request.session_id is not None:
            # éªŒè¯ä¼šè¯æ˜¯å¦å­˜åœ¨
            session = db_session.get_session(request.session_id)
            if not session:
                raise HTTPException(status_code=404, detail="Session not found")

            # è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt_param = db_config.get_parameter("system_prompt")
            system_prompt = system_prompt_param if system_prompt_param else config.DEFAULT_SYSTEM_PROMPT

            # ä¼°ç®—ç³»ç»Ÿæç¤ºè¯çš„ token æ•°
            system_prompt_tokens = estimate_message_tokens("system", system_prompt)

            # è·å– context_size å‚æ•°
            context_size_param = db_config.get_parameter("context_size")
            context_size = context_size_param if context_size_param else config.LLM_SERVER_CONTEXT_SIZE

            # è®¡ç®—å†å²è®°å½•çš„æœ€å¤§ token æ•°ï¼ˆcontext_size çš„ä¸€åŠï¼‰
            max_history_tokens = context_size // 2

            # ä»æ•°æ®åº“åŠ è½½å†å²æ¶ˆæ¯ï¼ˆåœ¨ token é™åˆ¶å†…ï¼‰
            history_messages = db_session.get_messages_within_token_limit(
                session_id=request.session_id,
                max_tokens=max_history_tokens,
                system_prompt_tokens=system_prompt_tokens
            )

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼š[system] + [history] + [new_user_message]
            messages_to_send.append({"role": "system", "content": system_prompt})

            # æ·»åŠ å†å²æ¶ˆæ¯
            for msg in history_messages:
                messages_to_send.append({"role": msg["role"], "content": msg["content"]})

            # æ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆåªå–æœ€åä¸€æ¡ï¼Œå› ä¸ºå‰ç«¯å¯èƒ½å‘é€å¤šæ¡ï¼‰
            if request.messages:
                new_user_message = request.messages[-1]
                messages_to_send.append({"role": new_user_message.role, "content": new_user_message.content})

                # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
                user_token_count = estimate_message_tokens(new_user_message.role, new_user_message.content)
                db_session.add_message(
                    session_id=request.session_id,
                    role=new_user_message.role,
                    content=new_user_message.content,
                    token_count=user_token_count
                )
        else:
            # æ— ä¼šè¯æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨è¯·æ±‚ä¸­çš„æ¶ˆæ¯
            messages_to_send = [{"role": msg.role, "content": msg.content} for msg in request.messages]

        # æµå¼å“åº”
        async def generate():
            assistant_response = ""  # æ”¶é›†å®Œæ•´çš„åŠ©æ‰‹å“åº”
            try:
                stream_gen = client.chat_stream(
                    messages=messages_to_send,
                    temperature=request.temperature,
                    repeat_penalty=request.repeat_penalty,
                    max_tokens=request.max_tokens
                )
                async for chunk in stream_gen:
                    # æ”¶é›†åŠ©æ‰‹å“åº”å†…å®¹
                    if "content" in chunk:
                        assistant_response += chunk["content"]

                    yield f"data: {json.dumps(chunk)}\n\n"

                yield "data: [DONE]\n\n"

                # å¦‚æœæ˜¯ä¼šè¯æ¨¡å¼ï¼Œä¿å­˜åŠ©æ‰‹å“åº”åˆ°æ•°æ®åº“
                if request.session_id is not None and assistant_response:
                    assistant_token_count = estimate_message_tokens("assistant", assistant_response)
                    db_session.add_message(
                        session_id=request.session_id,
                        role="assistant",
                        content=assistant_response,
                        token_count=assistant_token_count
                    )

            except Exception as e:
                error_data = {"error": str(e)}
                yield f"data: {json.dumps(error_data)}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except HTTPException:
        raise
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=config.API_SERVER_HOST, port=config.API_SERVER_PORT)
