#!/bin/bash

###############################################################################
# llama-server è‡ªåŠ¨å®‰è£…è„šæœ¬
#
# åŠŸèƒ½ï¼š
#   1. ä¸‹è½½ llama.cpp æºç 
#   2. ç¼–è¯‘ llama-serverï¼ˆæ”¯æŒ CPU/GPUï¼‰
#   3. å®‰è£…åˆ° ~/.local/bin
#   4. éªŒè¯å®‰è£…
###############################################################################

# set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

INSTALL_DIR="$HOME/.local/bin"
BUILD_DIR="/tmp/llama.cpp-build"
REPO_URL="https://github.com/ggerganov/llama.cpp.git"

echo "=========================================="
echo "ğŸš€ llama-server è‡ªåŠ¨å®‰è£…è„šæœ¬"
echo "=========================================="
echo ""

# æ£€æµ‹ GPU æ”¯æŒ
detect_gpu() {
    if command -v nvidia-smi &> /dev/null; then
        echo "âœ… æ£€æµ‹åˆ° NVIDIA GPU"
        USE_CUDA=ON
        GPU_LAYERS="-ngl 35"
    else
        echo "âš ï¸  æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œä½¿ç”¨ CPU ç‰ˆæœ¬"
        USE_CUDA=OFF
        GPU_LAYERS=""
    fi
}

# å®‰è£…ä¾èµ–
install_dependencies() {
    echo "ğŸ“¦ æ£€æŸ¥ä¾èµ–..."

    # æ£€æŸ¥å¿…è¦å·¥å…·
    for cmd in git cmake make g++; do
        if ! command -v $cmd &> /dev/null; then
            echo "âŒ ç¼ºå°‘ä¾èµ–: $cmd"
            echo "è¯·è¿è¡Œ: sudo apt-get install -y build-essential cmake git libcurl4-openssl-dev"
            exit 1
        fi
    done

    # æ£€æŸ¥ libcurl å¼€å‘åº“
    if ! pkg-config --exists libcurl 2>/dev/null; then
        echo "âŒ ç¼ºå°‘ä¾èµ–: libcurl-dev"
        echo "è¯·è¿è¡Œ: sudo apt-get install -y libcurl4-openssl-dev"
        exit 1
    fi

    echo "âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡"
}

# å…‹éš†ä»“åº“
clone_repo() {
    echo ""
    echo "ğŸ“¥ ä¸‹è½½ llama.cpp æºç ..."

    # æ¸…ç†æ—§çš„æ„å»ºç›®å½•
    if [ -d "$BUILD_DIR" ]; then
        echo "ğŸ§¹ æ¸…ç†æ—§çš„æ„å»ºç›®å½•..."
        rm -rf "$BUILD_DIR"
    fi

    # å…‹éš†æœ€æ–°ä»£ç 
    git clone --depth 1 "$REPO_URL" "$BUILD_DIR"
    cd "$BUILD_DIR"

    echo "âœ… æºç ä¸‹è½½å®Œæˆ"
}

# ç¼–è¯‘ llama-server
compile_server() {
    echo ""
    echo "ğŸ”¨ ç¼–è¯‘ llama-server..."

    cd "$BUILD_DIR"

    # åˆ›å»ºæ„å»ºç›®å½•
    mkdir -p build
    cd build

    # CMake é…ç½®
    if [ "$USE_CUDA" = "ON" ]; then
        echo "ğŸ® ä½¿ç”¨ CUDA åŠ é€Ÿç¼–è¯‘..."
        cmake .. -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release
    else
        echo "ğŸ’» ä½¿ç”¨ CPU ç¼–è¯‘..."
        cmake .. -DCMAKE_BUILD_TYPE=Release
    fi

    # ç¼–è¯‘ï¼ˆä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒï¼‰
    NPROC=$(nproc)
    echo "âš™ï¸  ä½¿ç”¨ $NPROC æ ¸å¿ƒå¹¶è¡Œç¼–è¯‘..."
    make llama-server -j$NPROC

    echo "âœ… ç¼–è¯‘å®Œæˆ"
}

# å®‰è£…åˆ°ç³»ç»Ÿ
install_binary() {
    echo ""
    echo "ğŸ“¦ å®‰è£… llama-server..."

    # ç¡®ä¿å®‰è£…ç›®å½•å­˜åœ¨
    mkdir -p "$INSTALL_DIR"
    mkdir -p "$HOME/.local/lib"

    # å¤åˆ¶å¯æ‰§è¡Œæ–‡ä»¶
    cp "$BUILD_DIR/build/bin/llama-server" "$INSTALL_DIR/"
    chmod +x "$INSTALL_DIR/llama-server"

    # å¤åˆ¶æ‰€æœ‰å…±äº«åº“åˆ° ~/.local/lib
    echo "ğŸ“š å¤åˆ¶å…±äº«åº“..."
    cp "$BUILD_DIR/build/bin/"*.so* "$HOME/.local/lib/" 2>/dev/null || true

    # åˆ›å»ºå¯åŠ¨åŒ…è£…è„šæœ¬ï¼ˆè®¾ç½® LD_LIBRARY_PATHï¼‰
    cat > "$INSTALL_DIR/llama-server-wrapper" <<'EOF'
#!/bin/bash
export LD_LIBRARY_PATH="$HOME/.local/lib:$LD_LIBRARY_PATH"
exec "$HOME/.local/bin/llama-server" "$@"
EOF
    chmod +x "$INSTALL_DIR/llama-server-wrapper"

    echo "âœ… å·²å®‰è£…åˆ°: $INSTALL_DIR/llama-server"
    echo "âœ… å…±äº«åº“å·²å®‰è£…åˆ°: $HOME/.local/lib"
    echo "ğŸ’¡ å»ºè®®ä½¿ç”¨åŒ…è£…è„šæœ¬: llama-server-wrapper"
}

# éªŒè¯å®‰è£…
verify_installation() {
    echo ""
    echo "ğŸ” éªŒè¯å®‰è£…..."

    if [ -f "$INSTALL_DIR/llama-server" ]; then
        VERSION=$("$INSTALL_DIR/llama-server" --version 2>&1 | head -1 || echo "unknown")
        echo "âœ… llama-server å®‰è£…æˆåŠŸ"
        echo "   ç‰ˆæœ¬: $VERSION"
        echo "   è·¯å¾„: $INSTALL_DIR/llama-server"

        # æ£€æŸ¥æ˜¯å¦åœ¨ PATH ä¸­
        if [[ ":$PATH:" == *":$INSTALL_DIR:"* ]]; then
            echo "âœ… $INSTALL_DIR å·²åœ¨ PATH ä¸­"
        else
            echo "âš ï¸  $INSTALL_DIR ä¸åœ¨ PATH ä¸­"
            echo "   è¯·æ·»åŠ åˆ° ~/.bashrc:"
            echo "   export PATH=\"\$HOME/.local/bin:\$PATH\""
        fi
    else
        echo "âŒ å®‰è£…å¤±è´¥"
        exit 1
    fi
}

# æ¸…ç†æ„å»ºç›®å½•
cleanup() {
    echo ""
    echo "ğŸ§¹ æ¸…ç†æ„å»ºæ–‡ä»¶..."
    rm -rf "$BUILD_DIR"
    echo "âœ… æ¸…ç†å®Œæˆ"
}

# ä¸»æµç¨‹
main() {
    # detect_gpu
    # install_dependencies
    # clone_repo
    # compile_server
    install_binary
    verify_installation
    # cleanup

    echo ""
    echo "=========================================="
    echo "ğŸ‰ llama-server å®‰è£…æˆåŠŸï¼"
    echo "=========================================="
    echo ""
    echo "ä½¿ç”¨æ–¹æ³•:"
    echo "  llama-server -m /path/to/model.gguf -c 4096 $GPU_LAYERS"
    echo ""
    echo "æŸ¥çœ‹å¸®åŠ©:"
    echo "  llama-server --help"
    echo ""
}

# è¿è¡Œä¸»æµç¨‹
main
